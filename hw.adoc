= Parallel Algorithms and Applications Homework Assignment
:author: Chakka Karthik Subramanyam
:doctype: article
:stylesheet: notebook.css


== Parallel Algorithms

Thanks to Moore's law, the conventional wisdom, suggested to wait for newer generation of processors to get more processing power.
However, we have reached a point, wherein we can't get more processing power by just increasing frequency, so manufacturers are pushed to increase the number of cores, to derive processing power.
Hence, the need to have exposure into parallel computing is more now than ever before.

Parallel computing though is the way forward, it's not a magic solution. It comes with it's own range of caveats and quirks.
For example,

* How will you divide the data between units?
* How to distribute data to each of the units?
* How are the dependencies between data/results handled?
* How to synchronize various parallel units?
* How reconstruction of the results of individual units occur?

Parallel algorithms answer the above questions(and hardware implementations and firmware/software framework apis enable the same).
Without having proper answers for the above questions, having more cores is not going to be of much use.
Hence, we can say that the parallel algorithms is the secret sauce that has realized high performance computing across domains.

Moreover, Parallelism is not restricted just to multiple processors running on a chip, it can happen at multiple levels:

* Between Instructions/Data on a single core. Ex: SIMD compiler Intrinsics
* Processors on a single chip. Ex: OpenMP, pthreads
* Machines/Cluster of machines on a network. Ex: MPI
* Custom/Specialized hardware. Ex: Programming GPGPUs, hardware accelerators etc.

There can also be hybrid solutions that can mix and match various architectures mentioned above.
Just like hardware adapt to various needs, the algorithms will need to adapt as well to make best use of hardware and other resources.

Without restricting ourselves to any specific parallel programming paradigms mentioned above, we'll provide a general idea which can be implemented across multiple paradigms(unless explicitly specified otherwise).

In the following section, we'll do a comparative theoretical analysis(in terms of time complexity) of how parallelization can improve the performance of sorting algorithms.
After that, we will do the same with few parallel algorithms related to linear algebra.

<<<

== Parallel Sorting Algorithms

Any sorting problem, can be represented as:

latexmath:[X = \{ x_1, x_2, .... , x_n \}]

set of n real numbers stored in an array of:

latexmath:[X[ \] = X[0\], X[1\], .... , X[n-1\]]

For latexmath:[n] elements, there are latexmath:[n!] ways that they can be arranged.
The goal is to find 1 permutation out of the latexmath:[n!] that abides to our desired order.

Below is the time complexity(Big O) of some of the conventional sequential sorting algorithms.
|===
|Name |Complexity(O(..)) |Comments

|Bubblesort
|latexmath:[n^2]
|Propagates the largest element towards the end of the array in each iteration

|Quicksort
|latexmath:[n log_2(n)]
|Recursive algorithm that sorts across a randomly chosen pivot

|Mergesort
|latexmath:[n log_2(n)]
|Divide and conquer algorithm that divides the array into smallest terminal units, and merges them back while sorting

|===

In the following sections we introduce various parallel sorting algorithms which are either modifications on the classical ones or unique to parallel paradigm.
We theoretically, evaluate the performance gains for each.
Some of the assumptions that we make:

* For latexmath:[P] processes, all data are already allocated
* All data are to be sorted in ascending order
* The number of processes latexmath:[P], is in powers of 2.

It's important to note that the best possible time complexity for a serial algorithm is:

latexmath:[O\left( n log_2(n) \right)]

for a parallelized algorithm it's:

latexmath:[O\left( log_2(n) \right)]

Thanks to parallelization we can reduce the complexity by a factor of latexmath:[n].
This holds true no matter how much units of latexmath:[n] we use.

In fact, practically, in our lab sessions have seen that we need to have proper balance between latexmath:[n] and latexmath:[P].
If we put too many processes, then much of time will be lost in synchronization and co-ordination of various processes, in worst cases we are better off using a serial algorithm.
If we use too less, then we are not exploiting the full potential of available resources.

Hence, it's all about finding that "sweet" spot.

<<<

=== Parallel Merge Sort
This is quite similar to the sequential merge sort. The tree structure of the algorithm, makes the distribution of the work between processes fairly simple.
This is illustrated in the figure below footnote:[source: R Rocha and F Silva (DCC-FCUP)]:

.Parallel Merge Sort
image::./pmsort.png[Parallel Merge Sort, 512]

==== Complexity analysis

For any merge sort, in the worst case:

* Number of steps to sort a sublist
+
latexmath:[2s - 1]
+
where latexmath:[s] is the size of the sublist:
+
latexmath:[s = 2^{i-1}]
+
where latexmath:[i] is the latexmath:[i^{th}] iteration.

* At any given step, number of sub-lists:
+
asciimath:[(n/s) = (n/2^(i-1))]
+
where latexmath:[n] is the total number of elements

* Number of steps in total:
+
latexmath:[log_2(n)]

In serial merge sort, the time complexity for latexmath:[i^{th}] step is:

latexmath:[ O\left( (\frac{n}{2^i}) (2^i - 1) \right) = O\left( n \right)]

Whereas in parallel merge sort the the time complexity for latexmath:[i^{th}] step is:

latexmath:[ O\left( 2^i - 1 \right) = O\left(2^i\right)]

We can clearly see that without parallelization, the time remains constant but proportional to size of initial array.
But, with parallelization, we have reduced the time, now it's in proportion to the subarray size.
This is because, at any given step all the latexmath:[( \frac{n}{2s})] sub-lists are processed in parallel.
Hence, complexity of Parallel merge sort:

latexmath:[O\left( \sum_{i=1}^{log_2(n)}( 2^i ) \right) = O(n)]

Therefore, we see a huge gain, wherein a relatively complex operation such as sorting is reduced to time complexity of a simpler operation such as, for example, say linear search.

<<<

=== Parallel Bubble Sort

A classic serial bubble sort, sorts the array by iteratively moving elements to their respective positions in relation to the end of the in each phase.

In parallel bubble sort the idea is to overlap multiple phases in such a way that the newly started phase doesn't disturb an on-going phase.
Considering the sequence of bubble sort, to achieve this, we need to pipeline the phases in the following manner footnote:[source: Clayton S Ferner, UNC Wilmington, Barry Wilkinson, UNC Charlotte]:

.Parallel Bubble Sort
image::pbub.png[Parallel Bubble Sort, 512]

==== Complexity analysis
In serial bubble sort, latexmath:[i^{th}] phase requires latexmath:[n - i] iterations and we'll have latexmath:[n - 1] such phases.
Hence, we have time complexity:

latexmath:[O\left( \sum_{i=1}^{n-1} n - i \right) = O\left( \frac{n(n-1)}{2}\right) = O\left( n^2\right)]

Now, thanks to parallelization, though each phase will take same time as in serial version, the end time will vary due to pipelining.
Consequently, latexmath:[i^{th}] phase will end at time:

latexmath:[n - 1 + i]

Hence, the end time of the algorithm and hence complexity is:

latexmath:[O\left( n - 1 + n - 1 \right) = O\left( 2n - 2 \right) = O\left( n \right)]

Hence, we have reduce complexity from latexmath:[n^2] to latexmath:[n].

<<<

=== Parallel Sort Regular Sampling

In this algorithm we can relax our last initial assumption.
Now latexmath:[P] need not be in powers of latexmath:[2], it can be any arbitrary natural number.
The algorithm is as follows:

* Sort local array using any algorithm.
If this algorithm is run on distributed network using let's say MPI, by using framework like OpenMP for local sorting higher throughput can be achieved.

* Sample P elements at intervals:
+
latexmath:[0, \frac{n}{P^2}, \frac{2n}{P^2}, ..., \frac{(P-1)n}{P^2}]

* Gather all these samples in another process and latexmath:[P-1] pivots and broadcast them to all the processes

* All processes partition their local array into latexmath:[P] pieces based on these pivots.

* Now each process latexmath:[P_i] retains the latexmath:[i^{th}] partition and sends it's latexmath:[j^{th}] partition to process latexmath:[P_i \forall j \neq i]

To illustrate with an example footnote:[source: Introduction to HPC with MPI for Data Science by Frank Nielsen]:

.Parallel Sort Regular Sampling
image::psrs.png[Parallel Sort Regular Sampling, 580]

In short, the intuition is: Sort locally and strategically share with each other parts of locally sorted array using some reference pivots selected among sorted samples.

==== Complexity Analysis
* Cost of local computations
** Local sort(assuming some efficient sequential algorithm was used):
+
latexmath:[O\left( \frac{n}{P} log_2(\frac{n}{P}) \right)]
** Sorting regular samples in intermediate step:
+
latexmath:[O\left( P^2 log_2(P^2) \right) = O\left( P^2 log_2(P) \right)]
** Merging sublists:
+
latexmath:[O\left( \frac{n}{P} log_2(P) \right)]
* Communication cost
** Gathering sample and broadcasting pivots:
+
latexmath:[\approx O\left( 1 \right)]
** Total exchange in the last step:
+
latexmath:[O\left( \frac{n}{P} \right)]

If we summate all the costs:

latexmath:[O\left( P^2 log_2(P) + \frac{n}{P} log_2(P) + \frac{n}{P} log_2(P) + 1 + \frac{n}{P} \right) = O\left( \frac{n}{P}(1 + \frac{1}{P^2}log_2(nP)) \right) = O(\frac{n}{P})]

Now, we have effectively reduced the time-complexity to latexmath:[t = \frac{n}{P}] where latexmath:[log_2(n) < t < n].

<<<

== Parallel Linear Algebra

In computer science, linear algebra plays a dominant role in many domains.
The most popular in the recent years is data science and machine learning.
Simple linear algebraic operations such as dot product of 2 vectors, matrix-vector & matrix-matrix multiplication are the major types of operations performed in these domains.
For ex:

* To evaluate the performance of model in machine learning, loss functions is used. It is nothing but a multi-dimensional root mean square operation.
* Back-propagation algorithm effectively is recursive dot product of partial derivatives across multiple layers in neural nets.

Though, the operations are simple, the scale at which they are performed is unprecedented.
For example: a simple 'hello world' introduction(MNIST digit recognition) into deep learning involves a model that requires several 256*256 matrix multiplications in one iteration of training.
Hence, both these domains are space and computation hungry.
With more and more industries adopting these technologies, more avenues in high performance computing are being explored to perform these operations in practical time scales.
Parallel algorithms provide the foundation which makes all these are practically possible/feasible.

Here we will consider 2 parallel Matrix-Matrix multiplication algorithms.

The conventional serial matrix multiplication has time complexity of:

latexmath:[O\left( n^3 \right)]

Though the order is in polynomial time, the scale at which this has to be executed, makes it totally impractical to use a naive serial algorithm.

Since, computations of each of the element in the result are independent we have good scope for parallelization.
If the size of the matrices can be accommodated within a RAM of a single machine, we can use a framework like OpenMP and achieve speed-up via shared-memory models.
However, if the order of the inputs are so large that they need to be distributed across machines, then we will need to use distributed algorithms.

<<<

=== Parallel Matrix Multiplication on distributed system
The algorithm consists of:

* Division of 3 matrices over the processes based on the rows i,e, each process is assigned latexmath:[\frac{n}{P}] rows.
+
.Data allocation footnote:[source: Arnaud Legrand, LIG, Grenoble]
image::ringdatadist.png[Data allocation, 320]
* In each iteration, every process perform matrix multiplication on it's local sub-matrix
+
.Local sub-matrix multiplication footnote:[source: Arnaud Legrand, LIG, Grenoble]
image::distlocsubmatmul.png[Local sub-matrix multiplication, 320]
* A virtual ring is formed in order to rotate rows between processes i,e, a process sends rows of B to the next process and receives rows of B from the previous process in the virtual ring.
+
.Virtual ring to pass rows footnote:[source: Arnaud Legrand, LIG, Grenoble, 320]
image::distmatmulring.png[Virtual ring to pass rows, 320]
* While the rows are rotated, the corresponding diagonals are rotated within each of the processes.

The algorithm is illustrated below :

.Stage 0 footnote:[source: Ned Nedialkov, McMaster University, Canada]
image::distalgos0.png[Stage 0, 480]

.Stage 1 footnote:[source: Ned Nedialkov, McMaster University, Canada]
image::distalgos1.png[Stage 1, 460]

.Stage 2 footnote:[source: Ned Nedialkov, McMaster University, Canada]
image::distalgos2.png[Stage 2, 460]

==== Complexity analysis
* There are latexmath:[P] steps.
* Each step takes time:
+
latexmath:[max(nr^2w, L + nrb)]
* Hence, running time:
+
latexmath:[P ( max(nr^2w, L + \frac{nr}{b})) = max(\frac{wn^3}{P}, L + P(\frac{n}{b}))]

There are 2 interesting things to note here:

* Big O
+
latexmath:[O\left( max(\frac{wn^3}{P}, L + P(\frac{n}{b})) \right) = O\left( \frac{wn^3}{P} \right)]
* Comparision with Vector Matrix Multiplication complexity(using same algorithm):
+
latexmath:[P ( max(nr^2w, nL + \frac{nr}{b}))]
+
Hence, by reduction in factor latexmath:[n], we have saved network latencies!

<<<

=== Parallel Matrix Multiplication within a system
Compared to the distributed version, this is fairly simple.
For the conventional triple nested for loop of sequential matrix multiplication:

.Serial Matrix Multiplication footnote:[source: Tristan Vanderbruggen & John Cavazos, University of Delaware]
image::sermat.png[Serial Matrix Multiplication, 280]

To achieve speed-up all we need to do is:

* fork as many threads as possible
* let threads share the matrices A, B, and C
* but make sure that the each thread has it's own iterators

All the above can be accomplished in just 2 lines of code with OpenMP footnote:[source: Tristan Vanderbruggen & John Cavazos, University of Delaware]:

.Parallel Matrix Multiplication OpenMP
image::openmpmatmul.png[Parallel Matrix Multiplication OpenMP, 370]

==== Complexity analysis
The serial matrix multiplication as we can see contains 3 nested for loops.
Then time complexity:

latexmath:[O\left( n^3 \right)]

If we assume the OpenMP directives create latexmath:[P] threads which would vectorize the for loop.
Then time complexity:

latexmath:[O\left( \frac{n^3}{P} \right)]