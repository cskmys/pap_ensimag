<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Asciidoctor 2.0.10">
<meta name="author" content="Chakka Karthik Subramanyam">
<title>Parallel Algorithms and Applications Homework Assignment</title>
<style>
/* Asciidoctor default stylesheet | MIT License | https://asciidoctor.org */

@import url(//fonts.googleapis.com/css?family=Cabin+Sketch|Architects+Daughter);
@import url(//asciidoctor.org/stylesheets/asciidoctor.css); /* Default asciidoc style framework - important */

/* CUSTOMISATIONS */
/* Change the values in root for quick customisation. If you want even more fine grain... venture further. */
:root{
--maincolor:#FFFFFF;
--primarycolor:#000000;
--secondarycolor:#1a237e;
--tertiarycolor:#CCCCCC;
--highlightcolor: #ffd600;
--sidebarbackground:#CACACA;
--linkcolor:#0D47A1;
--linkcoloralternate:#B71C1C;
--stickynote: #f9a825;
--white:#FFFFFF;
--black:#000000;
}

/* Text styles */

body{font-family: "Architects Daughter",sans-serif;background-color: #fff;background-image:linear-gradient(90deg, transparent 79px, #abced4 79px, #abced4 81px, transparent 81px),linear-gradient(#eee .15em, transparent .15em);background-size: 100% 1.2em;}

h1{color:var(--primarycolor) !important;font-family:"Cabin Sketch",sans-serif;}
h2,h3,h4,h5,h6{color:var(--secondarycolor) !important;font-family:"Cabin Sketch",sans-serif;}
.title{color:var(--black) !important;font-family:"Architects Daughter",sans-serif;font-style: normal; font-weight: normal;}
/*a{text-decoration: none;}*/
p{font-family: "Architects Daughter",sans-serif ! important}
#toc.toc2 a:link{color:var(--linkcolor); font-family: "Architects Daughter" !important}
blockquote{color:var(--secondarycolor) !important}
.quoteblock{color:var(--black)}
.quoteblock blockquote:before{color:var(--black)}
code{color:var(--highlightcolor);background-color: var(--black) !important}
mark{background-color: var(--highlightcolor)} /* Text highlighting color */
pre{background-color: var(--stickynote) !important;color:var(--secondarycolor);font-family: monospace;}

/* Table styles */
th{background-color: var(--maincolor);color:var(--black) !important;}
td{background-color: var(--maincolor);color: var(--black) !important}


#toc.toc2{background-color:var(--sidebarbackground);font-family: "Architects Daughter",sans-serif;}
#toctitle{color:var(--white); font-family: "Cabin Sketch"}

/* Responsiveness fixes */
video {
  max-width: 100%;
}

@media all and (max-width: 600px) {
table {
  width: 55vw!important;
  font-size: 3vw;
}

</style>
</head>
<body class="article">
<div id="header">
<h1>Parallel Algorithms and Applications Homework Assignment</h1>
<div class="details">
<span id="author" class="author">Chakka Karthik Subramanyam</span><br>
</div>
</div>
<div id="content">
<div class="sect1">
<h2 id="_parallel_algorithms">Parallel Algorithms</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Thanks to Moore&#8217;s law, the conventional wisdom, suggested to wait for newer generation of processors to get more processing power.
However, we have reached a point, wherein we can&#8217;t get more processing power by just increasing frequency, so manufacturers are pushed to increase the number of cores, to derive processing power.
Hence, the need to have exposure into parallel computing is more now than ever before.</p>
</div>
<div class="paragraph">
<p>Parallel computing though is the way forward, it&#8217;s not a magic solution. It comes with it&#8217;s own range of caveats and quirks.
For example,</p>
</div>
<div class="ulist">
<ul>
<li>
<p>How will you divide the data between units?</p>
</li>
<li>
<p>How to distribute data to each of the units?</p>
</li>
<li>
<p>How are the dependencies between data/results handled?</p>
</li>
<li>
<p>How to synchronize various parallel units?</p>
</li>
<li>
<p>How reconstruction of the results of individual units occur?</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Parallel algorithms answer the above questions(and hardware implementations and firmware/software framework apis enable the same).
Without having proper answers for the above questions, having more cores is not going to be of much use.
Hence, we can say that the parallel algorithms is the secret sauce that has realized high performance computing across domains.</p>
</div>
<div class="paragraph">
<p>Moreover, Parallelism is not restricted just to multiple processors running on a chip, it can happen at multiple levels:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Between Instructions/Data on a single core. Ex: SIMD compiler Intrinsics</p>
</li>
<li>
<p>Processors on a single chip. Ex: OpenMP, pthreads</p>
</li>
<li>
<p>Machines/Cluster of machines on a network. Ex: MPI</p>
</li>
<li>
<p>Custom/Specialized hardware. Ex: Programming GPGPUs, hardware accelerators etc.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>There can also be hybrid solutions that can mix and match various architectures mentioned above.
Just like hardware adapt to various needs, the algorithms will need to adapt as well to make best use of hardware and other resources.</p>
</div>
<div class="paragraph">
<p>Without restricting ourselves to any specific parallel programming paradigms mentioned above, we&#8217;ll provide a general idea which can be implemented across multiple paradigms(unless explicitly specified otherwise).</p>
</div>
<div class="paragraph">
<p>In the following section, we&#8217;ll do a comparative theoretical analysis(in terms of time complexity) of how parallelization can improve the performance of sorting algorithms.
After that, we will do the same with few parallel algorithms related to linear algebra.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_parallel_sorting_algorithms">Parallel Sorting Algorithms</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Any sorting problem, can be represented as:</p>
</div>
<div class="paragraph">
<p>\(X = \{ x_1, x_2, .... , x_n \}\)</p>
</div>
<div class="paragraph">
<p>set of n real numbers stored in an array of:</p>
</div>
<div class="paragraph">
<p>\(X[ ] = X[0], X[1], .... , X[n-1]\)</p>
</div>
<div class="paragraph">
<p>For \(n\) elements, there are \(n!\) ways that they can be arranged.
The goal is to find 1 permutation out of the \(n!\) that abides to our desired order.</p>
</div>
<div class="paragraph">
<p>Below is the time complexity(Big O) of some of the conventional sequential sorting algorithms.</p>
</div>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 33.3333%;">
<col style="width: 33.3333%;">
<col style="width: 33.3334%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Name</th>
<th class="tableblock halign-left valign-top">Complexity(O(..))</th>
<th class="tableblock halign-left valign-top">Comments</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Bubblesort</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">\(n^2\)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Propagates the largest element towards the end of the array in each iteration</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Quicksort</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">\(n log_2(n)\)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Recursive algorithm that sorts across a randomly chosen pivot</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Mergesort</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">\(n log_2(n)\)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Divide and conquer algorithm that divides the array into smallest terminal units, and merges them back while sorting</p></td>
</tr>
</tbody>
</table>
<div class="paragraph">
<p>In the following sections we introduce various parallel sorting algorithms which are either modifications on the classical ones or unique to parallel paradigm.
We theoretically, evaluate the performance gains for each.
Some of the assumptions that we make:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>For \(P\) processes, all data are already allocated</p>
</li>
<li>
<p>All data are to be sorted in ascending order</p>
</li>
<li>
<p>The number of processes \(P\), is in powers of 2.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>It&#8217;s important to note that the best possible time complexity for a serial algorithm is:</p>
</div>
<div class="paragraph">
<p>\(O\left( n log_2(n) \right)\)</p>
</div>
<div class="paragraph">
<p>for a parallelized algorithm it&#8217;s:</p>
</div>
<div class="paragraph">
<p>\(O\left( log_2(n) \right)\)</p>
</div>
<div class="paragraph">
<p>Thanks to parallelization we can reduce the complexity by a factor of \(n\).
This holds true no matter how much units of \(n\) we use.</p>
</div>
<div class="paragraph">
<p>In fact, practically, in our lab sessions have seen that we need to have proper balance between \(n\) and \(P\).
If we put too many processes, then much of time will be lost in synchronization and co-ordination of various processes, in worst cases we are better off using a serial algorithm.
If we use too less, then we are not exploiting the full potential of available resources.</p>
</div>
<div class="paragraph">
<p>Hence, it&#8217;s all about finding that "sweet" spot.</p>
</div>
<div class="sect2">
<h3 id="_parallel_merge_sort">Parallel Merge Sort</h3>
<div class="paragraph">
<p>This is quite similar to the sequential merge sort. The tree structure of the algorithm, makes the distribution of the work between processes fairly simple.
This is illustrated in the figure below <sup class="footnote">[<a id="_footnoteref_1" class="footnote" href="#_footnotedef_1" title="View footnote.">1</a>]</sup>:</p>
</div>
<div class="imageblock">
<div class="content">
<img src="./pmsort.png" alt="Parallel Merge Sort">
</div>
<div class="title">Figure 1. Parallel Merge Sort</div>
</div>
<div class="sect3">
<h4 id="_complexity_analysis">Complexity analysis</h4>
<div class="paragraph">
<p>For any merge sort, in the worst case:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Number of steps to sort a sublist</p>
<div class="paragraph">
<p>\(2s - 1\)</p>
</div>
<div class="paragraph">
<p>where \(s\) is the size of the sublist:</p>
</div>
<div class="paragraph">
<p>\(s = 2^{i-1}\)</p>
</div>
<div class="paragraph">
<p>where \(i\) is the \(i^{th}\) iteration.</p>
</div>
</li>
<li>
<p>At any given step, number of sub-lists:</p>
<div class="paragraph">
<p>\$(n/s) = (n/2^(i-1))\$</p>
</div>
<div class="paragraph">
<p>where \(n\) is the total number of elements</p>
</div>
</li>
<li>
<p>Number of steps in total:</p>
<div class="paragraph">
<p>\(log_2(n)\)</p>
</div>
</li>
</ul>
</div>
<div class="paragraph">
<p>In serial merge sort, the time complexity for \(i^{th}\) step is:</p>
</div>
<div class="paragraph">
<p>\( O\left( (\frac{n}{2^i}) (2^i - 1) \right) = O\left( n \right)\)</p>
</div>
<div class="paragraph">
<p>Whereas in parallel merge sort the the time complexity for \(i^{th}\) step is:</p>
</div>
<div class="paragraph">
<p>\( O\left( 2^i - 1 \right) = O\left(2^i\right)\)</p>
</div>
<div class="paragraph">
<p>We can clearly see that without parallelization, the time remains constant but proportional to size of initial array.
But, with parallelization, we have reduced the time, now it&#8217;s in proportion to the subarray size.
This is because, at any given step all the \(( \frac{n}{2s})\) sub-lists are processed in parallel.
Hence, complexity of Parallel merge sort:</p>
</div>
<div class="paragraph">
<p>\(O\left( \sum_{i=1}^{log_2(n)}( 2^i ) \right) = O(n)\)</p>
</div>
<div class="paragraph">
<p>Therefore, we see a huge gain, wherein a relatively complex operation such as sorting is reduced to time complexity of a simpler operation such as, for example, say linear search.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_parallel_bubble_sort">Parallel Bubble Sort</h3>
<div class="paragraph">
<p>A classic serial bubble sort, sorts the array by iteratively moving elements to their respective positions in relation to the end of the in each phase.</p>
</div>
<div class="paragraph">
<p>In parallel bubble sort the idea is to overlap multiple phases in such a way that the newly started phase doesn&#8217;t disturb an on-going phase.
Considering the sequence of bubble sort, to achieve this, we need to pipeline the phases in the following manner <sup class="footnote">[<a id="_footnoteref_2" class="footnote" href="#_footnotedef_2" title="View footnote.">2</a>]</sup>:</p>
</div>
<div class="imageblock">
<div class="content">
<img src="pbub.png" alt="Parallel Bubble Sort">
</div>
<div class="title">Figure 2. Parallel Bubble Sort</div>
</div>
<div class="sect3">
<h4 id="_complexity_analysis_2">Complexity analysis</h4>
<div class="paragraph">
<p>In serial bubble sort, \(i^{th}\) phase requires \(n - i\) iterations and we&#8217;ll have \(n - 1\) such phases.
Hence, we have time complexity:</p>
</div>
<div class="paragraph">
<p>\(O\left( \sum_{i=1}^{n-1} n - i \right) = O\left( \frac{n(n-1)}{2}\right) = O\left( n^2\right)\)</p>
</div>
<div class="paragraph">
<p>Now, thanks to parallelization, though each phase will take same time as in serial version, the end time will vary due to pipelining.
Consequently, \(i^{th}\) phase will end at time:</p>
</div>
<div class="paragraph">
<p>\(n - 1 + i\)</p>
</div>
<div class="paragraph">
<p>Hence, the end time of the algorithm and hence complexity is:</p>
</div>
<div class="paragraph">
<p>\(O\left( n - 1 + n - 1 \right) = O\left( 2n - 2 \right) = O\left( n \right)\)</p>
</div>
<div class="paragraph">
<p>Hence, we have reduce complexity from \(n^2\) to \(n\).</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_parallel_sort_regular_sampling">Parallel Sort Regular Sampling</h3>
<div class="paragraph">
<p>In this algorithm we can relax our last initial assumption.
Now \(P\) need not be in powers of \(2\), it can be any arbitrary natural number.
The algorithm is as follows:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Sort local array using any algorithm.
If this algorithm is run on distributed network using let&#8217;s say MPI, by using framework like OpenMP for local sorting higher throughput can be achieved.</p>
</li>
<li>
<p>Sample P elements at intervals:</p>
<div class="paragraph">
<p>\(0, \frac{n}{P^2}, \frac{2n}{P^2}, ..., \frac{(P-1)n}{P^2}\)</p>
</div>
</li>
<li>
<p>Gather all these samples in another process and \(P-1\) pivots and broadcast them to all the processes</p>
</li>
<li>
<p>All processes partition their local array into \(P\) pieces based on these pivots.</p>
</li>
<li>
<p>Now each process \(P_i\) retains the \(i^{th}\) partition and sends it&#8217;s \(j^{th}\) partition to process \(P_i \forall j \neq i\)</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>To illustrate with an example <sup class="footnote">[<a id="_footnoteref_3" class="footnote" href="#_footnotedef_3" title="View footnote.">3</a>]</sup>:</p>
</div>
<div class="imageblock">
<div class="content">
<img src="psrs.png" alt="Parallel Sort Regular Sampling">
</div>
<div class="title">Figure 3. Parallel Sort Regular Sampling</div>
</div>
<div class="paragraph">
<p>In short, the intuition is: Sort locally and strategically share with each other parts of locally sorted array using some reference pivots selected among sorted samples.</p>
</div>
<div class="sect3">
<h4 id="_complexity_analysis_3">Complexity Analysis</h4>
<div class="ulist">
<ul>
<li>
<p>Cost of local computations</p>
<div class="ulist">
<ul>
<li>
<p>Local sort(assuming some efficient sequential algorithm was used):</p>
<div class="paragraph">
<p>\(O\left( \frac{n}{P} log_2(\frac{n}{P}) \right)\)</p>
</div>
</li>
<li>
<p>Sorting regular samples in intermediate step:</p>
<div class="paragraph">
<p>\(O\left( P^2 log_2(P^2) \right) = O\left( P^2 log_2(P) \right)\)</p>
</div>
</li>
<li>
<p>Merging sublists:</p>
<div class="paragraph">
<p>\(O\left( \frac{n}{P} log_2(P) \right)\)</p>
</div>
</li>
</ul>
</div>
</li>
<li>
<p>Communication cost</p>
<div class="ulist">
<ul>
<li>
<p>Gathering sample and broadcasting pivots:</p>
<div class="paragraph">
<p>\(\approx O\left( 1 \right)\)</p>
</div>
</li>
<li>
<p>Total exchange in the last step:</p>
<div class="paragraph">
<p>\(O\left( \frac{n}{P} \right)\)</p>
</div>
</li>
</ul>
</div>
</li>
</ul>
</div>
<div class="paragraph">
<p>If we summate all the costs:</p>
</div>
<div class="paragraph">
<p>\(O\left( P^2 log_2(P) + \frac{n}{P} log_2(P) + \frac{n}{P} log_2(P) + 1 + \frac{n}{P} \right) = O\left( \frac{n}{P}(1 + \frac{1}{P^2}log_2(nP)) \right) = O(\frac{n}{P})\)</p>
</div>
<div class="paragraph">
<p>Now, we have effectively reduced the time-complexity to \(t = \frac{n}{P}\) where \(log_2(n) &lt; t &lt; n\).</p>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_parallel_linear_algebra">Parallel Linear Algebra</h2>
<div class="sectionbody">
<div class="paragraph">
<p>In computer science, linear algebra plays a dominant role in many domains.
The most popular in the recent years is data science and machine learning.
Simple linear algebraic operations such as dot product of 2 vectors, matrix-vector &amp; matrix-matrix multiplication are the major types of operations performed in these domains.
For ex:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>To evaluate the performance of model in machine learning, loss functions is used. It is nothing but a multi-dimensional root mean square operation.</p>
</li>
<li>
<p>Back-propagation algorithm effectively is recursive dot product of partial derivatives across multiple layers in neural nets.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Though, the operations are simple, the scale at which they are performed is unprecedented.
For example: a simple 'hello world' introduction(MNIST digit recognition) into deep learning involves a model that requires several 256*256 matrix multiplications in one iteration of training.
Hence, both these domains are space and computation hungry.
With more and more industries adopting these technologies, more avenues in high performance computing are being explored to perform these operations in practical time scales.
Parallel algorithms provide the foundation which makes all these are practically possible/feasible.</p>
</div>
<div class="paragraph">
<p>Here we will consider 2 parallel Matrix-Matrix multiplication algorithms.</p>
</div>
<div class="paragraph">
<p>The conventional serial matrix multiplication has time complexity of:</p>
</div>
<div class="paragraph">
<p>\(O\left( n^3 \right)\)</p>
</div>
<div class="paragraph">
<p>Though the order is in polynomial time, the scale at which this has to be executed, makes it totally impractical to use a naive serial algorithm.</p>
</div>
<div class="paragraph">
<p>Since, computations of each of the element in the result are independent we have good scope for parallelization.
If the size of the matrices can be accommodated within a RAM of a single machine, we can use a framework like OpenMP and achieve speed-up via shared-memory models.
However, if the order of the inputs are so large that they need to be distributed across machines, then we will need to use distributed algorithms.</p>
</div>
<div class="sect2">
<h3 id="_parallel_matrix_multiplication_on_distributed_system">Parallel Matrix Multiplication on distributed system</h3>
<div class="paragraph">
<p>The algorithm consists of:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Division of 3 matrices over the processes based on the rows i,e, each process is assigned \(\frac{n}{P}\) rows.</p>
<div class="imageblock">
<div class="content">
<img src="ringdatadist.png" alt="Data allocation">
</div>
<div class="title">Figure 4. Data allocation <sup class="footnote">[<a id="_footnoteref_4" class="footnote" href="#_footnotedef_4" title="View footnote.">4</a>]</sup></div>
</div>
</li>
<li>
<p>In each iteration, every process perform matrix multiplication on it&#8217;s local sub-matrix</p>
<div class="imageblock">
<div class="content">
<img src="distlocsubmatmul.png" alt="Local sub-matrix multiplication">
</div>
<div class="title">Figure 5. Local sub-matrix multiplication <sup class="footnote">[<a id="_footnoteref_5" class="footnote" href="#_footnotedef_5" title="View footnote.">5</a>]</sup></div>
</div>
</li>
<li>
<p>A virtual ring is formed in order to rotate rows between processes i,e, a process sends rows of B to the next process and receives rows of B from the previous process in the virtual ring.</p>
<div class="imageblock">
<div class="content">
<img src="distmatmulring.png" alt="Virtual ring to pass rows">
</div>
<div class="title">Figure 6. Virtual ring to pass rows <sup class="footnote">[<a id="_footnoteref_6" class="footnote" href="#_footnotedef_6" title="View footnote.">6</a>]</sup></div>
</div>
</li>
<li>
<p>While the rows are rotated, the corresponding diagonals are rotated within each of the processes.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The algorithm is illustrated below :</p>
</div>
<div class="imageblock">
<div class="content">
<img src="distalgos0.png" alt="Stage 0">
</div>
<div class="title">Figure 7. Stage 0 <sup class="footnote">[<a id="_footnoteref_7" class="footnote" href="#_footnotedef_7" title="View footnote.">7</a>]</sup></div>
</div>
<div class="imageblock">
<div class="content">
<img src="distalgos1.png" alt="Stage 1">
</div>
<div class="title">Figure 8. Stage 1 <sup class="footnote">[<a id="_footnoteref_8" class="footnote" href="#_footnotedef_8" title="View footnote.">8</a>]</sup></div>
</div>
<div class="imageblock">
<div class="content">
<img src="distalgos2.png" alt="Stage 2">
</div>
<div class="title">Figure 9. Stage 2 <sup class="footnote">[<a id="_footnoteref_9" class="footnote" href="#_footnotedef_9" title="View footnote.">9</a>]</sup></div>
</div>
<div class="sect3">
<h4 id="_complexity_analysis_4">Complexity analysis</h4>
<div class="paragraph">
<p>As we saw from the example,</p>
</div>
<div class="ulist">
<ul>
<li>
<p>There are \(P\) steps.</p>
</li>
<li>
<p>Each step takes time:</p>
<div class="paragraph">
<p>\(max(nr^2w, L + nrb)\)</p>
</div>
</li>
<li>
<p>Hence, running time:</p>
<div class="paragraph">
<p>\(P ( max(nr^2w, L + \frac{nr}{b})) = max(\frac{wn^3}{P}, L + P(\frac{n}{b}))\)</p>
</div>
</li>
</ul>
</div>
<div class="paragraph">
<p>There are 2 things to note here:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Big O</p>
<div class="paragraph">
<p>\(O\left( max(\frac{wn^3}{P}, L + P(\frac{n}{b})) \right) = O\left( \frac{wn^3}{P} \right)\)</p>
</div>
</li>
<li>
<p>Comparision with Matrix vector complexity:</p>
<div class="paragraph">
<p>\(P ( max(nr^2w, nL + \frac{nr}{b}))\)</p>
</div>
<div class="paragraph">
<p>is the running time of matrix vector multiplication using the same algorithm.
Hence, by reduction in factor \(n\), we have saved network latencies!</p>
</div>
</li>
</ul>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_parallel_matrix_multiplication_within_a_system">Parallel Matrix Multiplication within a system</h3>
<div class="paragraph">
<p>Compared to the distributed version, this is fairly simple.
For the conventional triple nested for loop of sequential matrix multiplication:</p>
</div>
<div class="imageblock">
<div class="content">
<img src="sermat.png" alt="Serial Matrix Multiplication">
</div>
<div class="title">Figure 10. Serial Matrix Multiplication <sup class="footnote">[<a id="_footnoteref_10" class="footnote" href="#_footnotedef_10" title="View footnote.">10</a>]</sup></div>
</div>
<div class="paragraph">
<p>To achieve speed-up all we need to do is:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>fork as many threads as possible</p>
</li>
<li>
<p>let threads share the matrices A, B, and C</p>
</li>
<li>
<p>but make sure that the each thread has it&#8217;s own iterators</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>All the above can be accomplished in just 2 lines of code with OpenMP <sup class="footnote">[<a id="_footnoteref_11" class="footnote" href="#_footnotedef_11" title="View footnote.">11</a>]</sup>:</p>
</div>
<div class="imageblock">
<div class="content">
<img src="openmpmatmul.png" alt="Parallel Matrix Multiplication OpenMP">
</div>
<div class="title">Figure 11. Parallel Matrix Multiplication OpenMP</div>
</div>
<div class="sect3">
<h4 id="_complexity_analysis_5">Complexity analysis</h4>
<div class="paragraph">
<p>The serial matrix multiplication as we can see contains 3 nested for loops, each iterating for say, \(n\) dimension.
Then time complexity:</p>
</div>
<div class="paragraph">
<p>\(O\left( n^3 \right)\)</p>
</div>
<div class="paragraph">
<p>If we assume the OpenMP directives create \(P\) threads which would vectorize the for loop.
Then time complexity:</p>
</div>
<div class="paragraph">
<p>\(O\left( \frac{n^3}{P} \right)\)</p>
</div>
</div>
</div>
</div>
</div>
</div>
<div id="footnotes">
<hr>
<div class="footnote" id="_footnotedef_1">
<a href="#_footnoteref_1">1</a>. source: R Rocha and F Silva (DCC-FCUP)
</div>
<div class="footnote" id="_footnotedef_2">
<a href="#_footnoteref_2">2</a>. source: Clayton S Ferner, UNC Wilmington, Barry Wilkinson, UNC Charlotte
</div>
<div class="footnote" id="_footnotedef_3">
<a href="#_footnoteref_3">3</a>. source: Introduction to HPC with MPI for Data Science by Frank Nielsen
</div>
<div class="footnote" id="_footnotedef_4">
<a href="#_footnoteref_4">4</a>. source: Arnaud Legrand, LIG, Grenoble
</div>
<div class="footnote" id="_footnotedef_5">
<a href="#_footnoteref_5">5</a>. source: Arnaud Legrand, LIG, Grenoble
</div>
<div class="footnote" id="_footnotedef_6">
<a href="#_footnoteref_6">6</a>. source: Arnaud Legrand, LIG, Grenoble
</div>
<div class="footnote" id="_footnotedef_7">
<a href="#_footnoteref_7">7</a>. source: Ned Nedialkov, McMaster University, Canada
</div>
<div class="footnote" id="_footnotedef_8">
<a href="#_footnoteref_8">8</a>. source: Ned Nedialkov, McMaster University, Canada
</div>
<div class="footnote" id="_footnotedef_9">
<a href="#_footnoteref_9">9</a>. source: Ned Nedialkov, McMaster University, Canada
</div>
<div class="footnote" id="_footnotedef_10">
<a href="#_footnoteref_10">10</a>. source: Tristan Vanderbruggen &amp; John Cavazos, University of Delaware
</div>
<div class="footnote" id="_footnotedef_11">
<a href="#_footnoteref_11">11</a>. source: Tristan Vanderbruggen &amp; John Cavazos, University of Delaware
</div>
</div>
<div id="footer">
<div id="footer-text">
Last updated 2020-05-15 12:55:04 +0200
</div>
</div>
</body>
</html>